{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X87D8Wsz90sD"
   },
   "source": [
    "# **Kickstarter Success Exploratory Data Analysis**\n",
    "**BUSI/COMP 488-002 Final Project** | May 7, 2021\n",
    "\n",
    "*Brendan Carr  \n",
    "Daniel Tracy  \n",
    "Kevin Barth  \n",
    "Siddharth Bowgal  \n",
    "Alex Damiano  \n",
    "Peter Morrow*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OPINlpLQOvtY"
   },
   "source": [
    "### Objective & Overview\n",
    "\n",
    "___\n",
    "\n",
    "We were tasked with creating a strategy for identifying successful Kickstarter campaigns so that investors can choose the projects with the most promise. To do this, we analyzed which features of a Kickstarter project available at the project start date were most useful in determining success. We also looked for projects with an amount of money pledged that was over the goal in order to determine which projects were the best investments. Finally, we performed a text analysis to identify key adjectives in the successful projects, to give investors further insight into what kinds of language help promote a project toward success.\n",
    "\n",
    "#### Outline\n",
    "\n",
    "\n",
    "1. Set Up Notebook & Load Data\n",
    "2. Data Transformation\n",
    "3. Comparing Models for Classification\n",
    "4. Does Model Generalize Well?\n",
    "5. Predicting Amount over Goal\n",
    "6. Appendix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XLShLL6LO29H"
   },
   "source": [
    "### 1. Set Up Notebook & Load Data\n",
    "\n",
    "___\n",
    "\n",
    "First, we will import some fundamental libraries and read in our raw data.\n",
    "\n",
    "> **Note:** In order to run this notebook, you must download the ***ks-projects-201801.csv.zip*** file inlcuded on the [Github page](https://github.com/brendancarr34/Kickstarter-project-EDA) and unzip it.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T-IfWkf5-7ar"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from datetime import date\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# This class is used for aesthetic output purposes of this notebook.\n",
    "class color:\n",
    "  BOLD = '\\033[1m'\n",
    "  END = '\\033[0m'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 374
    },
    "id": "VBr7rrMR_KPk",
    "outputId": "9309de16-c9d5-42de-f318-0b29a55f93b2"
   },
   "outputs": [],
   "source": [
    "# Filepath may need to be changed depending on the location of the csv file\n",
    "\n",
    "df = pd.read_csv(r\"C:\\Users\\xxxx\\Desktop\\ks-projects-201801.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JOjLlX5MAAQ0"
   },
   "source": [
    "#### Data Preview\n",
    "\n",
    "We take a quick snapshot of the data to get a feel for the imported data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q2QpoWGD_vPm"
   },
   "outputs": [],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vzk7QHH8B2Eq"
   },
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gk4YXmyOARcP"
   },
   "source": [
    "### 2. Data Transformation\n",
    "\n",
    "___\n",
    "\n",
    "Given such a large dataset, there will be variables and data we do not want to include our analysis. This simplifies the results of our EDA and gives actionable insights into predicting success of certain projects. In this section, we prime our data for analysis using the folowing strategies:\n",
    "- Dropping Columns\n",
    "- Filtering Rows\n",
    "- Visualizing Outliers and Correlation\n",
    "- Removing Outliers\n",
    "- Data Type Casting\n",
    "- Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BVUpIwe5ApiV"
   },
   "source": [
    "#### Dropping Columns\n",
    "\n",
    "To simplify our analysis, we will look at only US data - therefore our recommendations will be for US based projects. We also need to drop ID and name since these are just identifiers - there is no real information inherent in these features. We also notice there are multiple variations of the \"pledged\" variable; since we are only working with US data, we will only keep \"pledged\" and drop the rest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zKzbmxvmObYn"
   },
   "outputs": [],
   "source": [
    "# Drop ID and name\n",
    "try:\n",
    "  df.drop(labels = ['ID'], axis = 1, inplace = True)\n",
    "except:\n",
    "  print(\"Already dropped 'id' and 'name' columns.\")\n",
    "\n",
    "# Filter on 'currency' = 'USD' and drop 'currency'\n",
    "try:\n",
    "  df = df[df.currency == 'USD']\n",
    "  df.drop(labels = ['currency'], axis = 1, inplace = True)\n",
    "except:\n",
    "  print(\"Already dropped 'currency' column.\")\n",
    "\n",
    "# Filter on country = US and drop country\n",
    "try:\n",
    "  df = df[df.country == 'US']\n",
    "  df.drop(labels = ['country'], axis = 1, inplace = True)\n",
    "except:\n",
    "  print(\"Already dropped 'country' column.\")\n",
    "\n",
    "# Drop usd pledged, usd_pledged_real, and usd_goal_real\n",
    "try:\n",
    "  df.drop(labels = ['usd pledged','usd_pledged_real', 'usd_goal_real'], axis = 1, inplace = True)\n",
    "except:\n",
    "  print(\"Already dropped 'usd pledged','usd_pledged_real', and 'usd_goal_real' columns.\")\n",
    "\n",
    "print(df.shape)\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qih_UI_SxUrR"
   },
   "source": [
    "#### Filtering Rows\n",
    "\n",
    "We want to look only at completed campaigns, so we can train our classifier on data that had a certain outcome (shown in the 'state' variable). We also filter on projects with a goal under $50,000 since that seemed to be a reasonable goal, and we look at medium-term projects (at most 1 year term).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GDkn5iZ8AUgp"
   },
   "outputs": [],
   "source": [
    "print(df.state.value_counts())\n",
    "\n",
    "# Filter out postings that are still active or undefined\n",
    "df = df[df.state != \"undefined\"]\n",
    "df = df[df.state != \"live\"]\n",
    "df.state.value_counts()\n",
    "\n",
    "# Drop everything under 50k\n",
    "df.loc[df.goal > 50000,'goal']=None\n",
    "# Drop everything over 1 year in length\n",
    "df.loc[df.term >365, 'term']=None\n",
    "# Drop outliers over 50k in funding, to \n",
    "df.loc[df.pledged > 50000, 'pledged']=None\n",
    "df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m_uXrZLg5utF"
   },
   "outputs": [],
   "source": [
    "# Confirm that active and undefined projects have been removed.\n",
    "print(df.state.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Uv3QiVQQ2sYd"
   },
   "source": [
    "#### Visualizing Outliers and Correlation\n",
    "\n",
    "In this section, we visualize the data to get a better understanding of outliers, trends, and correlation. We modify the state variable to make all projects that are not \"successful\" to be considered \"unsuccessful\". Again, this is for completed campaigns - those that were labeled \"active\" or \"undefined\" were removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YSd1YXjEZnQJ"
   },
   "outputs": [],
   "source": [
    "temp_df = df\n",
    "temp_df.state =  temp_df.state.astype('string')\n",
    "for index, row in temp_df.iterrows():\n",
    "  if (row['state'] != 'successful'):\n",
    "    temp_df.at[index, 'state'] = 'unsuccessful'\n",
    "repmap={\"successful\": 1, \"unsuccessful\": 0}\n",
    "temp_df['state'].replace(repmap, inplace=True)\n",
    "temp_df.state= temp_df.state.astype('uint8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "NEk1sGMgZqZU",
    "outputId": "9a061c07-afa3-49ec-b9ff-1fbc4aa606b3"
   },
   "outputs": [],
   "source": [
    "fig, axarr = plt.subplots(4, 2, figsize=(30, 40))\n",
    "(temp_df[temp_df['state']==1]).main_category.value_counts().plot(kind='bar', ax=axarr[0][0])\n",
    "temp_df.groupby('main_category')['state'].mean().plot(kind='bar', ax=axarr[0][1])\n",
    "temp_df.groupby('year')['state'].mean().plot(kind='bar', ax=axarr[1][0])\n",
    "temp_df.groupby('mnth_lnch')['state'].mean().plot(kind='bar', ax=axarr[1][1])\n",
    "temp_df.groupby('dow_lnch')['state'].mean().plot(kind='bar', ax=axarr[2][0])\n",
    "temp_df.groupby('hour_lnch')['state'].mean().plot(kind='bar', ax=axarr[2][1])\n",
    "temp_df.groupby('mnth_ddln')['state'].mean().plot(kind='bar', ax=axarr[3][0])\n",
    "temp_df.groupby('term')['state'].mean().plot( ax=axarr[3][1])\n",
    "facet = sns.FacetGrid(temp_df, hue=\"state\",aspect=4)\n",
    "facet.map(sns.kdeplot, 'goal', shade= True)\n",
    "facet.set(xlim=(temp_df['goal'].min(), temp_df['goal'].max()))\n",
    "facet.add_legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 705
    },
    "id": "aTUgDJIWOJP7",
    "outputId": "a34c4e08-9066-4eb1-b67b-1a67bd7fde51"
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "plt.subplots(figsize=(18,12))\n",
    "sns.heatmap(df.corr(), annot=True, cmap=\"Blues\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b5Lsclkp3Yki"
   },
   "source": [
    "#### Remove Outliers\n",
    "\n",
    "To identify possible outliers, we can create two visuals: histograms and boxplots. The histogram shows us how frequent a value occurs in a bin, while a boxplot shows the minimum, maximum, and IQR (interquartile range) of a dataset. After looking at our visualizations to get a better understanding of outliers, we filter the data even further by keeping campaigns with a term under 61 days. Finally, after we confirm that we have no missing data, our data is ready for analysis using machine learning.\n",
    "\n",
    "If you don't reach your goal, you get none of the money... we want projects to be successful so we will filter down to 10k goals since these are even more attainable..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8JIlviMnA7EL"
   },
   "outputs": [],
   "source": [
    "# REVIEW\n",
    "\n",
    "# Drop everything under 10k\n",
    "df = df.loc[df.goal < 10000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 809
    },
    "id": "yR8y5QHHCJYj",
    "outputId": "daf9f049-a7a2-48ed-90b8-3cdaf9e90c67"
   },
   "outputs": [],
   "source": [
    "# filter out outliers \n",
    "# Display distribution of numeric features to observe outliers.\n",
    "\n",
    "\n",
    "numeric_features =['goal', 'pledged', 'term']\n",
    "for col in numeric_features:\n",
    "  f, axes = plt.subplots(1, 2, figsize=(8, 4)) \n",
    "  df[col].hist(bins = 30, ax = axes[0])\n",
    "  axes[0].set_title('Distribution of '+ col)\n",
    "  df.boxplot(column = col, ax = axes[1])\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cJQRch-tD2qm",
    "outputId": "2fdbb88d-5fc4-4c0c-b084-89bf48732d3d"
   },
   "outputs": [],
   "source": [
    "# change to 10k?\n",
    "print('% under 25k goal: ' + str((df['goal']<25000).sum()/len(df)*100))\n",
    "print('% under 50k funded: ' + str((df['pledged']<50000).sum()/len(df)*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xdmBjdceEwcQ"
   },
   "outputs": [],
   "source": [
    "# Drop everything over 61 days in length\n",
    "df = df.loc[df.term < 61]\n",
    "# Drop outliers over 50k in funding, to \n",
    "df = df.loc[df.pledged < 50000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 809
    },
    "id": "SNTVyZP8D0i1",
    "outputId": "8bddfa4f-de5d-4190-a03b-1ec1e1cda135"
   },
   "outputs": [],
   "source": [
    "# Run visualization again to see distribution\n",
    "numeric_features =['goal', 'pledged', 'term']\n",
    "for col in numeric_features:\n",
    "  f, axes = plt.subplots(1, 2, figsize=(8, 4)) \n",
    "  df[col].hist(bins = 30, ax = axes[0])\n",
    "  axes[0].set_title('Distribution of '+ col)\n",
    "  df.boxplot(column = col, ax = axes[1])\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yi-OmrQQBt4G",
    "outputId": "d86d42a0-1539-44a2-98c0-3623da56286a"
   },
   "outputs": [],
   "source": [
    "# check for missing data\n",
    "print(df.isnull().sum())\n",
    "print(df.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fM_EKxtd3QNp"
   },
   "source": [
    "#### Data Type Casting\n",
    "\n",
    "We type cast our variables appropriately so they can be properly read by machine learning models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CXg8uEO9CxMj"
   },
   "outputs": [],
   "source": [
    "#Set Categorical data to category type\n",
    "df.state = df.state.astype('category')\n",
    "df.dow_lnch = df.dow_lnch.astype('category')\n",
    "df.main_category = df.main_category.astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "l-Q7cFwZSbUO",
    "outputId": "6aaca576-45d2-44e7-c59e-95b7a5d2c747"
   },
   "outputs": [],
   "source": [
    "df['mnth_lnch']=df['mnth_lnch'].astype('category')\n",
    "df['mnth_ddln']=df['mnth_ddln'].astype('category')\n",
    "df['hour_lnch']=df['hour_lnch'].astype('category')\n",
    "df['5friends'] = df['5friends'].astype('uint8')\n",
    "df['10friends'] = df['10friends'].astype('uint8')\n",
    "df['15friends'] = df['15friends'].astype('uint8')\n",
    "print(df.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EdIVNGN6gjLX"
   },
   "source": [
    "#### Feature Engineering\n",
    "\n",
    "We create features for year, month, day, and hour using Pandas DatetimeIndex as potential predictors. We also create the feature \"term\" to replace the \"deadline\" and \"launched\" features since this information can be expressed in one feature. Finally, we drop the features that were used to engineer the new features, to avoid multicollinearity.\n",
    "\n",
    "**Definitions of key new variables:**\n",
    "\n",
    "1. **5friends** = Does the project have at least 5 backers?\n",
    "    * Expressed as a binary categorical variable (1 for Yes, 0 for No)\n",
    "    * Similar variables created: **10friends** and **15friends**  \n",
    "    \n",
    "\n",
    "2. **Over_goal_50** = Does the project have a pledged amount that is at least 150% of the goal?\n",
    "    * Expressed as a binary categorical variable (1 for Yes, 0 for No)\n",
    "    * Similar variables created: **Over_goal_20** and **Over_goal_30**  \n",
    "    \n",
    "\n",
    "3. **Predicted_state** = These are predictions of whether a project will be successful using the Random Forest classifier we trained and refined.\n",
    "    * Expressed as a binary categorical variable (1 for Yes, 0 for No)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_mnIV2Vs-hTj"
   },
   "outputs": [],
   "source": [
    "# add month, day of the week, and hour of posting\n",
    "df['deadline'] = pd.to_datetime(df['deadline'])\n",
    "df['launched'] = pd.to_datetime(df['launched'])\n",
    "df['year'] = pd.DatetimeIndex(df['launched']).year\n",
    "df['mnth_lnch'] = pd.DatetimeIndex(df['launched']).month\n",
    "df['dow_lnch']=df['launched'].dt.day_name()\n",
    "df['hour_lnch'] = pd.DatetimeIndex(df['launched']).hour\n",
    "df['mnth_ddln'] = pd.DatetimeIndex(df['deadline']).month\n",
    "\n",
    "# add term length\n",
    "df['term'] = df['deadline'] - df['launched']\n",
    "df['term'] = pd.TimedeltaIndex(df['term']).days\n",
    "\n",
    "#drop unnecessary columns\n",
    "df.drop(['deadline', 'launched', 'category'], axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hRx6XYL56VwL"
   },
   "outputs": [],
   "source": [
    "# adding categorical variables for backers greater than 5, 10, and 15\n",
    "df['5friends'] = np.where(df['backers'] >= 5, 1, 0)\n",
    "df['10friends'] = np.where(df['backers'] >= 10, 1, 0)\n",
    "df['15friends'] = np.where(df['backers'] >= 15, 1, 0)\n",
    "print(df['5friends'].value_counts())\n",
    "print(df['10friends'].value_counts())\n",
    "print(df['15friends'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E4UEupZc4iFw"
   },
   "source": [
    "### 3. Comparing Models for Classification\n",
    "\n",
    "___\n",
    "\n",
    "We want to compare different classifiers to see what works best with our data, and use the winner for further analysis. First we will prepare the data set for the machine learning algorithms. The models we evaluate are the following:\n",
    "\n",
    "*   Random Forest\n",
    "*   Support Vector Machine\n",
    "*   Logistic Regression\n",
    "*   Decision Tree Classifier (with AdaBoost)\n",
    "*   Bagging Classifier\n",
    "*   K-Nearest Neighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eBYSSbYlwOYq"
   },
   "source": [
    "#### Prepare Data for Machine Learning\n",
    "\n",
    "We will use 2015 data to evaluate different classifiers. Choosing 2015 was an arbitrary subset of the data - we will check if the chosen model generalizes to other years' data later in the notebook.\n",
    "\n",
    "Preparing the data involves dropping \"5friends\" and \"15friends\", and keeping \"10friends\". For further explanation on why we went about preparing the data this way, please refer to the Appendix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rXOz_7j8eH03"
   },
   "outputs": [],
   "source": [
    "# Define functions to evaluate our models\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def show_results(y_test, y_pred):\n",
    "\n",
    "  # Output the accuracy of our prediction\n",
    "  print(f\"R-square = {round(accuracy_score(y_test, y_pred),4)}\")\n",
    "\n",
    "  # Visualize the confusion matrix to make it easier to read\n",
    "  con_matrix = confusion_matrix(y_test, y_pred)\n",
    "  confusion_matrix_df = pd.DataFrame(con_matrix, ('Unsuccessful', 'Successful'), ('Unsuccessful', 'Successful'))\n",
    "  heatmap = sns.heatmap(confusion_matrix_df, annot=True, annot_kws={\"size\": 20}, fmt=\"d\", cmap=\"Blues\")\n",
    "  heatmap.yaxis.set_ticklabels(heatmap.yaxis.get_ticklabels(), rotation=0, ha='right', fontsize = 14)\n",
    "  heatmap.xaxis.set_ticklabels(heatmap.xaxis.get_ticklabels(), rotation=45, ha='right', fontsize = 14)\n",
    "  plt.ylabel('Actual', fontsize = 14)\n",
    "  plt.xlabel('Predicted', fontsize = 14)\n",
    "\n",
    "  # Print the classification report\n",
    "  from sklearn.metrics import classification_report\n",
    "  print(classification_report(y_test, y_pred))\n",
    "\n",
    "def best_model(model):\n",
    "    print(model.best_score_)    \n",
    "    print(model.best_params_)\n",
    "    print(model.best_estimator_)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XFD84ZlBwCcu"
   },
   "outputs": [],
   "source": [
    "# Create evaluate_model function\n",
    "def evaluate_model(predictions, probs, train_predictions, train_probs):\n",
    "    \"\"\"Compare machine learning model to baseline performance.\n",
    "    Computes statistics and shows ROC curve.\"\"\"\n",
    "    \n",
    "    baseline = {}\n",
    "    \n",
    "    baseline['recall'] = recall_score(y_test, [1 for _ in range(len(y_test))])\n",
    "    baseline['precision'] = precision_score(y_test, [1 for _ in range(len(y_test))])\n",
    "    baseline['roc'] = 0.5\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    results['recall'] = recall_score(y_test, predictions)\n",
    "    results['precision'] = precision_score(y_test, predictions)\n",
    "    results['roc'] = roc_auc_score(y_test, probs)\n",
    "    \n",
    "    train_results = {}\n",
    "    train_results['recall'] = recall_score(y_sm, train_predictions)\n",
    "    train_results['precision'] = precision_score(y_sm, train_predictions)\n",
    "    train_results['roc'] = roc_auc_score(y_sm, train_probs)\n",
    "    \n",
    "    for metric in ['recall', 'precision', 'roc']:\n",
    "        print(f'{metric.capitalize()} Test: {round(results[metric], 2)} Train: {round(train_results[metric], 2)}')\n",
    "    \n",
    "    # Calculate false positive rates and true positive rates\n",
    "    base_fpr, base_tpr, _ = roc_curve(y_test, [1 for _ in range(len(y_test))])\n",
    "    model_fpr, model_tpr, _ = roc_curve(y_test, probs)\n",
    "\n",
    "    plt.figure(figsize = (8, 6))\n",
    "    plt.rcParams['font.size'] = 16\n",
    "    \n",
    "    # Plot both curves\n",
    "    plt.plot(base_fpr, base_tpr, 'b', label = 'baseline')\n",
    "    plt.plot(model_fpr, model_tpr, 'r', label = 'model')\n",
    "    plt.legend();\n",
    "    plt.xlabel('False Positive Rate'); plt.ylabel('True Positive Rate'); plt.title('ROC Curves');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 419
    },
    "id": "bGp7V0j3vgKX",
    "outputId": "080f2f1d-f817-4441-8fb1-e9e397d3b733"
   },
   "outputs": [],
   "source": [
    "# Drop highly influential variables, keep only \"10friends\"\n",
    "df_classifier2015 = df[df.year== 2015]\n",
    "df_classifier2015 = df_classifier2015.drop(['pledged','year','backers','name'], axis = 1)\n",
    "\n",
    "df_classifier2015 = df_classifier2015.drop(['5friends','15friends'], axis = 1)\n",
    "\n",
    "display(df_classifier2015)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XzKzFWZRvs-G",
    "outputId": "08ef6ef4-3316-4d7f-e68d-d8e85e0d6537"
   },
   "outputs": [],
   "source": [
    "df_classifier2015.state.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KiBVnU5dvudI",
    "outputId": "03dab919-0c29-488b-c066-29c393298d06"
   },
   "outputs": [],
   "source": [
    "# We typecast variables accordingly and make sure all data types are correct.\n",
    "df_classifier2015['state'] = df_classifier2015['state'].astype('category')\n",
    "df_classifier2015.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 479
    },
    "id": "7WsweZPpvzKf",
    "outputId": "7c8c600e-6459-4b30-c6cf-85c456d985aa"
   },
   "outputs": [],
   "source": [
    "# Subset the data into predictor variables (X) and the target variable (y)\n",
    "X = df_classifier2015.loc[:, df_classifier2015.columns != 'state']\n",
    "\n",
    "y = df_classifier2015.state\n",
    "\n",
    "# Now we need to one hot encode the categorical features to make them machine readable. \n",
    "X = pd.get_dummies(X)\n",
    "\n",
    "print(f\"{color.BOLD}Predictor Variables for Kickstarter Data{color.END} - {X.shape[1]} columns x {X.shape[0]:,d} rows\\n\")\n",
    "\n",
    "display(X.head())\n",
    "try: \n",
    "  repmap={\"successful\": 1, \"unsuccessful\": 0}\n",
    "  y.replace(repmap, inplace=True)\n",
    "except:\n",
    "  print(f\"\\n{color.BOLD}~~~{color.END} 'state' already encoded during this runtime execution. {color.BOLD}~~~{color.END}\\n\")\n",
    "\n",
    "print(f\"{color.BOLD}Target Variable for Kickstarter Data{color.END} - {y.shape[0]:,d} rows\\n\")\n",
    "display(y.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 334
    },
    "id": "-rkUKdv9v3Jg",
    "outputId": "609febc1-1729-4618-fa4f-869304d9b863"
   },
   "outputs": [],
   "source": [
    "# We want to scale our continuous numeric data to optimize our models\n",
    "\n",
    "def scale_numeric(features, numeric_features, scaler):\n",
    "    for col in numeric_features:\n",
    "        features[col] = scaler.fit_transform(features[col].values.reshape(-1, 1))\n",
    "    return features\n",
    "\n",
    "numeric_features = ['goal','term']\n",
    "\n",
    "#2 we can now define the scaler we want to use and apply it to our features \n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "scaler = MinMaxScaler()\n",
    "X_scaled = scale_numeric(X, numeric_features, scaler)\n",
    "\n",
    "#3 Let's see if it worked\n",
    "X_scaled.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "79JHTzOCv6kN"
   },
   "outputs": [],
   "source": [
    "# Basic imports\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split data into 70% train and 30% test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y,\n",
    "test_size=0.3,\n",
    "stratify=y,\n",
    "random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_EOuVuIkzCjX"
   },
   "source": [
    "We need to take care of class imbalance - we will use the SMOTE package to do so\n",
    "\n",
    "> Source: https://www.analyticsvidhya.com/blog/2020/10/overcoming-class-imbalance-using-smote-techniques/\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DgStrUtuv_Lh",
    "outputId": "6d89c611-f5d5-4621-b721-6fadf09745c5"
   },
   "outputs": [],
   "source": [
    "# Handles class imbalance\n",
    "# Source: https://www.analyticsvidhya.com/blog/2020/10/overcoming-class-imbalance-using-smote-techniques/\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from collections import Counter\n",
    "counter= Counter(y)\n",
    "print('Before', counter)\n",
    "\n",
    "smt=SMOTE()\n",
    "x_sm, y_sm = smt.fit_resample(X_train, y_train)\n",
    "counter= Counter(y_sm)\n",
    "print('After', counter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nJXYvbbldvbH"
   },
   "source": [
    "#### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 518
    },
    "id": "POsEo1oBb_6F",
    "outputId": "3646aaff-f3f1-42c5-ecf9-d57aeccd14d9"
   },
   "outputs": [],
   "source": [
    "# Random Forest\n",
    "\n",
    "# Instantiate a random forests classifier, let's call it 'rf'\n",
    "rf = RandomForestClassifier(n_estimators=25, \n",
    "                            bootstrap = True, \n",
    "                            max_features = 'auto', \n",
    "                            min_samples_leaf = 5, \n",
    "                            criterion='gini',\n",
    "                            random_state=42)\n",
    "\n",
    "# Fit 'rf' to the training set\n",
    "rf.fit(x_sm, y_sm)\n",
    "\n",
    "# Predict the test set labels 'y_pred'\n",
    "y_pred = rf.predict(X_test)\n",
    "show_results(y_test, y_pred)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 521
    },
    "id": "9d1AQ-ihxPab",
    "outputId": "96f8622c-4fde-40ca-c0fc-8334a9342b04"
   },
   "outputs": [],
   "source": [
    "# Let's evaluate our Tree's performance using AUC\n",
    "# Make probability predictions\n",
    "train_probs = rf.predict_proba(x_sm)[:, 1]\n",
    "probs = rf.predict_proba(X_test)[:, 1]\n",
    "\n",
    "train_predictions = rf.predict(x_sm)\n",
    "predictions = rf.predict(X_test)\n",
    "\n",
    "from sklearn.metrics import precision_score, recall_score, roc_auc_score, roc_curve\n",
    "print(f'Train ROC AUC Score: {roc_auc_score(y_sm, train_probs)}')\n",
    "print(f'Test ROC AUC  Score: {roc_auc_score(y_test, probs)}')\n",
    "print(f'Baseline ROC AUC: {roc_auc_score(y_test, [1 for _ in range(len(y_test))])}')\n",
    "\n",
    "# Call our ROC evaluation function\n",
    "evaluate_model(predictions, probs, train_predictions, train_probs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I8PduY5seJJg"
   },
   "source": [
    "#### Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 572
    },
    "id": "f_YvmPE6foJR",
    "outputId": "28dffb17-a364-483c-d6ee-d42491daa6f7"
   },
   "outputs": [],
   "source": [
    "#Create a svm Classifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "svmachine = SVC(C=100, gamma=0.1, kernel='poly', max_iter=1000, random_state=42) \n",
    "     # c=regulariziation (penalty), gamma=fitting (over vs under), kernel=transformation function\n",
    "\n",
    "#Train the model using the training sets\n",
    "svmachine.fit(x_sm, y_sm)\n",
    "\n",
    "#Predict the response for test dataset\n",
    "y_pred = svmachine.predict(X_test)\n",
    "show_results(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QdgCN4rPf4Qz"
   },
   "source": [
    "#### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2T9YbPBNf8wk"
   },
   "outputs": [],
   "source": [
    "# Logistic Regression\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Fit primal logistic regression\n",
    "param_grid = {'C': [10,50,100,200,300], 'max_iter': [1000], 'fit_intercept':[True],'intercept_scaling':[1],\n",
    "              'penalty':['l2'], 'tol':[0.001,0.0001,0.00001]}\n",
    "log_primal_Grid = GridSearchCV(LogisticRegression(solver='lbfgs', random_state=42),param_grid, cv=5, refit=True, verbose=0)\n",
    "log_primal_Grid.fit(x_sm, y_sm);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wSJxZKuGiex_",
    "outputId": "17e5d456-ef5d-4dd6-e02a-337ecc4542a3"
   },
   "outputs": [],
   "source": [
    "best_model(log_primal_Grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 518
    },
    "id": "M5OtQKZvivDS",
    "outputId": "28b99a67-9952-4c63-c58d-b7ac08b867f9"
   },
   "outputs": [],
   "source": [
    "y_pred = log_primal_Grid.predict(X_test)\n",
    "show_results(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3CPc5h5dlKf5"
   },
   "source": [
    "#### Decision Tree Classifier (with AdaBoost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 518
    },
    "id": "mTNRdZqmlPFe",
    "outputId": "0eea90a6-c0bc-4c52-bdde-adf0830dec0c"
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "# Instantiate a classification-tree, let's call it 'dt'\n",
    "dt = DecisionTreeClassifier(max_depth=1, criterion='gini', min_samples_leaf = 10, splitter = \"random\")\n",
    "\n",
    "# Instantiate an AdaBoost classifier, let's call it 'adab_clf'\n",
    "adb_clf = AdaBoostClassifier(base_estimator=dt, n_estimators=25,random_state=42, learning_rate=.1)\n",
    "\n",
    "# Fit 'adb_clf' to the training set\n",
    "adb_clf.fit(x_sm, y_sm)\n",
    "\n",
    "# Predict the test data\n",
    "y_pred = adb_clf.predict(X_test)\n",
    "show_results(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Eysluqz9ldyS"
   },
   "source": [
    "#### Bagging Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GDmUJtJ3lbRT",
    "outputId": "c3c51862-3ee7-4ca7-feec-f416b276da97"
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "\n",
    "# Instantiate a classification-tree, let's call it 'dt'\n",
    "dt = DecisionTreeClassifier(criterion='gini', random_state = 22)\n",
    "\n",
    "dt.fit(x_sm,y_sm)\n",
    "\n",
    "# Predict test-set labels\n",
    "y_pred= dt.predict(X_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print('Accuracy of Decision Classifier: {:.3f}'.format(accuracy))\n",
    "\n",
    "# Instantiate a BaggingClassifier, let's call it 'bc'\n",
    "bc = BaggingClassifier(base_estimator=dt, n_estimators=25, n_jobs=-1,random_state=22)\n",
    "\n",
    "# Fit 'bc' to the training set\n",
    "bc.fit(x_sm, y_sm)\n",
    "\n",
    "# Predict test set labels\n",
    "y_pred = bc.predict(X_test)\n",
    "\n",
    "# Evaluate and print test-set accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print('Accuracy of Bagging Classifier: {:.3f}'.format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qt6LlGbIT3Kv"
   },
   "source": [
    "#### K-Nearest Neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RT3XDE7aT55n",
    "outputId": "217918c9-3438-4c07-da4e-39adde29e13f"
   },
   "outputs": [],
   "source": [
    "# import the k-nearest neighbors classifier from sci-kit learn\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Instantiate the KNeighborsClassifier with a n_neighbors value of 3\n",
    "knn = KNeighborsClassifier(n_neighbors=3)\n",
    "\n",
    "# Fit the model\n",
    "knn.fit(x_sm, y_sm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "X-q98s7uUait",
    "outputId": "333c28b0-b853-4683-cb4f-64bcb9435221"
   },
   "outputs": [],
   "source": [
    "# # Run prediction on test data\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"Test set predictions: \\n {}\".format(y_pred))\n",
    "\n",
    "# # Calculate the accuracy of our prediction using np.mean\n",
    "print(\"Accuracy of Predicition: {:.2f}\".format(np.mean(y_pred==y_test)))\n",
    "\n",
    "# # Alternatively, we can use knn's internal score function\n",
    "print(\"Accuracy of Predicition: {:2f}\".format(knn.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fb1qKvuUxSh1"
   },
   "source": [
    "We evaluate our models using confusion matrices and accuracy scores. After comparing models and some team discussion, we decide to go with Random Forest as our best model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SgfuAtWqOB5O"
   },
   "source": [
    "### 4. Does Model Generalize Well?\n",
    "\n",
    "___\n",
    "\n",
    "\n",
    "In the previous section, we tested and evaluated models on a subset of our data, namely 2015 data. We need to verify that our chosen model, Random Forest, works well on the other data so that we remain confident in its ability to predict success of a campaign."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XxkO218raxAy"
   },
   "source": [
    "#### Generalize to 20XX data\n",
    "\n",
    "We can look at data from different years to see if the model generalizes well. In this example, we test the Random Forest on 2017 data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 419
    },
    "id": "ioML5sH1a29l",
    "outputId": "250ad05b-4dc1-4bc2-860f-84dfebb44c29"
   },
   "outputs": [],
   "source": [
    "df_classifier2017 = df[df.year== 2017]\n",
    "df_classifier2017 = df_classifier2017.drop(['pledged','year','backers','name'], axis = 1)\n",
    "\n",
    "df_classifier2017 = df_classifier2017.drop(['5friends','15friends'], axis = 1)\n",
    "\n",
    "display(df_classifier2017)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dXXdP6sjbJaU",
    "outputId": "c6f8df7f-e676-4632-f23a-f02ed554112b"
   },
   "outputs": [],
   "source": [
    "df_classifier2017.state.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Sctcx97tbMnJ",
    "outputId": "7da6be55-b4b0-4ea0-ee48-921622555349"
   },
   "outputs": [],
   "source": [
    "df_classifier2017['state'] = df_classifier2017['state'].astype('category')\n",
    "df_classifier2017.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 479
    },
    "id": "JANx1dcRbbVS",
    "outputId": "6c7cf5fc-3001-41c2-83fe-9175edafd830"
   },
   "outputs": [],
   "source": [
    "X = df_classifier2017.loc[:, df_classifier2017.columns != 'state']\n",
    "\n",
    "y = df_classifier2017.state\n",
    "\n",
    "# Now you need to one hot encode the categorical features to make them machine readable. \n",
    "X = pd.get_dummies(X)\n",
    "\n",
    "print(f\"{color.BOLD}Predictor Variables for Kickstarter Data{color.END} - {X.shape[1]} columns x {X.shape[0]:,d} rows\\n\")\n",
    "\n",
    "display(X.head())\n",
    "try: \n",
    "  repmap={\"successful\": 1, \"unsuccessful\": 0}\n",
    "  y.replace(repmap, inplace=True)\n",
    "except:\n",
    "  print(f\"\\n{color.BOLD}~~~{color.END} 'state' already encoded during this runtime execution. {color.BOLD}~~~{color.END}\\n\")\n",
    "\n",
    "print(f\"{color.BOLD}Target Variable for Kickstarter Data{color.END} - {y.shape[0]:,d} rows\\n\")\n",
    "display(y.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 334
    },
    "id": "4g6JaaXPbcBq",
    "outputId": "edae2153-b96d-427b-e533-6f0fdd3e36dd"
   },
   "outputs": [],
   "source": [
    "#2 we can now define the scaler we want to use and apply it to our features \n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "scaler = MinMaxScaler()\n",
    "X_scaled = scale_numeric(X, numeric_features, scaler)\n",
    "\n",
    "#3 Let's see if it worked\n",
    "X_scaled.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 523
    },
    "id": "EyzHwf6KbtBo",
    "outputId": "3ab72f41-c0c4-42f6-df8b-c5c70fc10b24"
   },
   "outputs": [],
   "source": [
    "# Here, we use the Random Forest already created in the \"Random Forest\" section\n",
    "\n",
    "# Predict the test set labels 'y_pred'\n",
    "y_pred = rf.predict(X_scaled)\n",
    "show_results(y, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yC7I1-jwbA19"
   },
   "source": [
    "### 5. Predicting Amount over Goal\n",
    "\n",
    "___\n",
    "\n",
    "Campaigns with especially strong funding are great indicators of the strong faith that backers have in a particular project. We believe that projects with a pledged amount that is 150% of a project's goal shows remarkable interest in a project, and we want to be able to identify these types of campaigns for investors.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HJkfW2KOtt5R"
   },
   "source": [
    "#### Preparing Data for Over 50% Excess Funding\n",
    "\n",
    "We prepare the data the same way it was prepared in the \"Prepare Data for Machine Learning\" section, except we do it for our entire dataset. We also create a new column, \"predicted_state\", to create predictions for whether a project is successful using our Random Forest classifier that we created earlier. We then add this column to our Dataframe.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 419
    },
    "id": "8tOoTldLbeOZ",
    "outputId": "228bb955-c0a8-4dea-d928-2c01d988bb03"
   },
   "outputs": [],
   "source": [
    "df_classifierOverGoal = df.drop(['pledged','year','backers','name'], axis = 1)\n",
    "\n",
    "df_classifierOverGoal = df_classifierOverGoal.drop(['5friends','15friends'], axis = 1)\n",
    "\n",
    "display(df_classifierOverGoal)\n",
    "\n",
    "df_classifierOverGoal['state'] = df_classifierOverGoal['state'].astype('category')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 479
    },
    "id": "iVpkHc-wbvjI",
    "outputId": "a6d2f4d3-e3e0-4734-feff-2e622730eed7"
   },
   "outputs": [],
   "source": [
    "X = df_classifierOverGoal.loc[:, df_classifierOverGoal.columns != 'state']\n",
    "\n",
    "y = df_classifierOverGoal.state\n",
    "\n",
    "# Now we need to one hot encode the categorical features to make them machine readable. \n",
    "X = pd.get_dummies(X)\n",
    "\n",
    "print(f\"{color.BOLD}Predictor Variables for Kickstarter Data{color.END} - {X.shape[1]} columns x {X.shape[0]:,d} rows\\n\")\n",
    "\n",
    "display(X.head())\n",
    "try: \n",
    "  repmap={\"successful\": 1, \"unsuccessful\": 0}\n",
    "  y.replace(repmap, inplace=True)\n",
    "except:\n",
    "  print(f\"\\n{color.BOLD}~~~{color.END} 'state' already encoded during this runtime execution. {color.BOLD}~~~{color.END}\\n\")\n",
    "\n",
    "print(f\"{color.BOLD}Target Variable for Kickstarter Data{color.END} - {y.shape[0]:,d} rows\\n\")\n",
    "display(y.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4-XNjyTHcDVp"
   },
   "outputs": [],
   "source": [
    "numeric_features = ['goal','term']\n",
    "\n",
    "#2 we can now define the scaler we want to use and apply it to our features \n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "scaler = MinMaxScaler()\n",
    "X_scaled = scale_numeric(X, numeric_features, scaler)\n",
    "\n",
    "#3 Let's see if it worked\n",
    "X_scaled.describe()\n",
    "\n",
    "predicted_values = rf.predict(X_scaled)\n",
    "df['predicted_state'] = predicted_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 456
    },
    "id": "leMXNauzcVN6",
    "outputId": "74ddd3b6-2ee1-4487-d858-c56ba5516cc8"
   },
   "outputs": [],
   "source": [
    "display(X_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K6LWr3aMt1SY"
   },
   "source": [
    "#### Excess Funding Feature Engineering\n",
    "\n",
    "We create the following columns and add them to our dataset: 'over_goal_50', 'over_goal_20', 'over_goal_30'. These are binary variables that take on the value of 1 for projects that have pledged amounts in excess of goal of 50%, 20%, and 30% respectively. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 359
    },
    "id": "1qdQaeUP56OF",
    "outputId": "effc0ee0-19cb-4781-8e11-734342573458"
   },
   "outputs": [],
   "source": [
    "def getOverGoal50(df):\n",
    "  df['over_goal_50'] = (df['pledged']/df['goal'] > 1.5)\n",
    "  return df\n",
    "\n",
    "def getOverGoal20(df):\n",
    "  df['over_goal_20'] = (df['pledged']/df['goal'] > 1.2)\n",
    "  return df\n",
    "\n",
    "def getOverGoal30(df):\n",
    "  df['over_goal_30'] = (df['pledged']/df['goal'] > 1.3)\n",
    "  return df\n",
    "\n",
    "\n",
    "# Add the columns.\n",
    "df = getOverGoal50(df)\n",
    "df = getOverGoal20(df)\n",
    "df = getOverGoal30(df)\n",
    "\n",
    "# Map them to 0 or 1\n",
    "repmap={True: 1, False: 0}\n",
    "df['over_goal_50'].replace(repmap, inplace=True)\n",
    "df['over_goal_20'].replace(repmap, inplace=True)\n",
    "df['over_goal_30'].replace(repmap, inplace=True)\n",
    "\n",
    "# Quickly see if our new columns have indeed been added to the data\n",
    "display(df[['pledged','goal','over_goal_50','over_goal_20','over_goal_30']].sample(n=10, random_state = 42))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 796
    },
    "id": "4rHVJELaf0Az",
    "outputId": "80f61832-dea9-4bc1-bb63-eacf66cfaf79"
   },
   "outputs": [],
   "source": [
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tPDlz33bt93f"
   },
   "source": [
    "#### Creating X and Y for Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IdFEi9yy_irL"
   },
   "source": [
    "\n",
    "\n",
    "After performing scenario analysis to see which of the three \"over_goal\" variables we should use as our target variable, we settled on using 'over_goal_50', so we dropped the other two.\n",
    "\n",
    "\n",
    "To build on the success of our Random Forest classifier, we subset the data based on how our Random Forest predicted the success of a project (i.e. where 'predicted_state' = 1), and ran another Random Forest on this new subset to predict whether a project will have a 'pledged' value that is over 50% in excess of its goal.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cDPPbeJB7wsF",
    "outputId": "fbe66bc6-b79a-4ef9-9c42-7c95d63e8a0b"
   },
   "outputs": [],
   "source": [
    "\n",
    "pred_Successful = df[df['predicted_state'] == 1]\n",
    "print(pred_Successful.columns)\n",
    "print(pred_Successful.dtypes)\n",
    "pred_Successful = pred_Successful.drop(columns=['predicted_state','pledged','state','backers','over_goal_20','over_goal_30', '5friends','15friends','name'])\n",
    "\n",
    "pred_Successful = pd.get_dummies(pred_Successful)\n",
    "\n",
    "overgoal_rf = pred_Successful.copy()\n",
    "\n",
    "overgoal_rf_Y = overgoal_rf.over_goal_50\n",
    "\n",
    "overgoal_rf_X = overgoal_rf.loc[:, overgoal_rf.columns != 'over_goal_50']\n",
    "\n",
    "print(overgoal_rf_X.columns)\n",
    "print(overgoal_rf_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 456
    },
    "id": "DpXRJnQEkrgO",
    "outputId": "3c2c5aa2-aeae-4e9f-9f3f-011a870a4eae"
   },
   "outputs": [],
   "source": [
    "display(overgoal_rf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pEPfqIWQuE_s"
   },
   "source": [
    "#### Classifier Output\n",
    "\n",
    "We create a new Random Forest classifier and train it on our new subset of data, to see how well it predicts 'over_goal_50'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 555
    },
    "id": "iDk-lDpX_mbv",
    "outputId": "278a0e65-5633-41be-e897-593089d01d3f"
   },
   "outputs": [],
   "source": [
    "# Basic imports\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split data into 70% train and 30% test\n",
    "X_train, X_test, y_train, y_test = train_test_split(overgoal_rf_X, overgoal_rf_Y,\n",
    "test_size=0.3,\n",
    "random_state=42)\n",
    "\n",
    "# Instantiate a random forests classifier\n",
    "rf = RandomForestClassifier(n_estimators=25, \n",
    "                            bootstrap = True, \n",
    "                            max_features = 'auto', \n",
    "                            min_samples_leaf = 5, \n",
    "                            criterion='gini',\n",
    "                            random_state=42)\n",
    "\n",
    "# Fit 'rf' to the training set\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# Predict the test set labels 'y_pred'\n",
    "y_pred = rf.predict(X_test)\n",
    "\n",
    "# Output the accuracy of our prediction\n",
    "from sklearn.metrics import accuracy_score\n",
    "print(accuracy_score(y_test, y_pred))\n",
    "\n",
    "# Visualize the confusion matrix to make it easier to read\n",
    "con_matrix = confusion_matrix(y_test, y_pred)\n",
    "confusion_matrix_df = pd.DataFrame(con_matrix, ('Over Goal By < 50%', 'Over Goal By > 50%'), ('Over Goal By < 50%', 'Over Goal By > 50%'))\n",
    "heatmap = sns.heatmap(confusion_matrix_df, annot=True, annot_kws={\"size\": 20}, fmt=\"d\", cmap=\"Blues\")\n",
    "heatmap.yaxis.set_ticklabels(heatmap.yaxis.get_ticklabels(), rotation=0, ha='right', fontsize = 14)\n",
    "heatmap.xaxis.set_ticklabels(heatmap.xaxis.get_ticklabels(), rotation=45, ha='right', fontsize = 14)\n",
    "plt.ylabel('Actual', fontsize = 14)\n",
    "plt.xlabel('Predicted', fontsize = 14)\n",
    "\n",
    "# Print the classification report\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x0Pn7tOABeDz"
   },
   "source": [
    "To sum up, we first used a Random Forest classifier to predict whether a campaign will be successful. Then, we make a new Random Forest classifier to be used on the projects that were deemed successful by the first Random Forest, to predict whether a project will rake in pledged money that is over 50% in excess of the project's goal. Evaluating the combination of the two classifiers, we arrive at the following result:\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "U-8lKkY7pzaX",
    "outputId": "8c99f46f-9a86-4a24-c433-7387573eee86"
   },
   "outputs": [],
   "source": [
    "df_successful = df[df.predicted_state == 1]\n",
    "#print(df.state.value_counts())\n",
    "predicted_over50_values = rf.predict(overgoal_rf_X)\n",
    "df_successful['over50'] = predicted_over50_values\n",
    "df_successful = df_successful[df_successful['over50']==1]\n",
    "df_successful.state = df_successful.state.astype('uint8')\n",
    "\n",
    "print(f'Of the campaigns that we predicted to be successful and reach 150% of their goal, {color.BOLD}{df_successful.state.mean()*100}%{color.END} were actually successful\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9dgk5xuythet"
   },
   "source": [
    "#### Visualization of Predicted Successful and Over 50% Funding in Excess of Goal\n",
    "\n",
    "In this section, we create visualizations of our final dataset that contains predictions made by our two Random Forest classifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "fa1iveN59vA8",
    "outputId": "c95411ed-8862-4567-9b1a-d2e1065941c7"
   },
   "outputs": [],
   "source": [
    "fig, axarr = plt.subplots(4, 2, figsize=(30, 40))\n",
    "df_successful.main_category.value_counts().plot(kind='bar', ax=axarr[0][0])\n",
    "df_successful.groupby('year')['state'].sum().plot(kind='bar', ax=axarr[1][0])\n",
    "df_successful.groupby('mnth_lnch')['state'].sum().plot(kind='bar', ax=axarr[1][1])\n",
    "df_successful.groupby('dow_lnch')['state'].sum().plot(kind='bar', ax=axarr[2][0])\n",
    "df_successful.groupby('hour_lnch')['state'].sum().plot(kind='bar', ax=axarr[2][1])\n",
    "df_successful.groupby('mnth_ddln')['state'].sum().plot(kind='bar', ax=axarr[3][0])\n",
    "df_successful.groupby('term')['state'].sum().plot( ax=axarr[3][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wrW4Or8rBQBI"
   },
   "source": [
    "### 6. Appendix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TQoERqEjaKHi"
   },
   "source": [
    "#### 10 Friends Reasoning\n",
    "\n",
    "Knowing that individuals are less likely to give if no one else has done so, we wanted to find a way to quanitify this momentum effect. To do with we looked at data w less than 50 backers to compare all postings vs. those with more than 10 backers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2YgswXZFrDlr",
    "outputId": "994a0aa1-687e-45b7-b04a-d92b8a7c06df"
   },
   "outputs": [],
   "source": [
    "df_10friends = df[df.backers>10]\n",
    "df_10friends=df_10friends[df.backers<50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a-yIBjJur1iH"
   },
   "outputs": [],
   "source": [
    "df_no10friends=df[df.backers<50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 289
    },
    "id": "A2rvhw9grKN5",
    "outputId": "b4717d7c-5f88-4a6c-b367-14a7d18eb83d"
   },
   "outputs": [],
   "source": [
    "df_10friends.backers.plot(kind='box')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 287
    },
    "id": "THx9FB0qsfMf",
    "outputId": "492e5bd1-8cd9-4d05-8e53-3a8baac01512"
   },
   "outputs": [],
   "source": [
    "df_no10friends.backers.plot(kind='box')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HoQuHY8DrlDt"
   },
   "source": [
    "From this, we see that it is far more difficult to get from 0 to 10 backers than 10 to 20. When filtering only those less than 50, the avg number of backers is less than 10, but once 10 is reached, the avg number of backers is 25. This shows the importance of momentum, and how important the first few backers are in rallying others to give money towards a project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WvyU9EPvZDut"
   },
   "source": [
    "#### Text Analysis of Successful Projects\n",
    "\n",
    "Finally, to understand how to best phrase a posting, we used text analysis to find the top 10 nouns and modifiers used in the names of successful posts for each category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FUkM8ECiN8JD",
    "outputId": "9403df58-7965-4c03-a47b-f7bea95fc285"
   },
   "outputs": [],
   "source": [
    "# importing package for text analysis\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "import spacy\n",
    "nlp = spacy.load('en')\n",
    "from nltk.tokenize import word_tokenize\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 512
    },
    "id": "Cb4xaz_oEgEm",
    "outputId": "cf4547f4-2b49-40a3-e0f1-367c7c83baa8"
   },
   "outputs": [],
   "source": [
    "# Loops through data and selects only nouns from successful posts of each category\n",
    "# Takes 7-8 mins to run\n",
    "adjective_counter = pd.DataFrame()\n",
    "for cat in df.main_category.unique():\n",
    "  string = ''\n",
    "  for index, row in df.iterrows():\n",
    "    if row['main_category']==cat:\n",
    "      if row['state']==1:\n",
    "        string= string+' ' +row['name']\n",
    "  BoW = []\n",
    "  doc = nlp(string)\n",
    "  for token in doc:\n",
    "    if token.dep_ == 'amod':\n",
    "      BoW.append(token.text)\n",
    "\n",
    "  \n",
    "  adjective_counter[cat]=Counter(no_punct).most_common(10)\n",
    "\n",
    "display(adjective_counter)\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 512
    },
    "id": "xGWgCSN858Ku",
    "outputId": "85c9995b-b178-4bd6-95fe-bad739407a5c"
   },
   "outputs": [],
   "source": [
    "# Loops through data and selects only nouns from successful posts of each category\n",
    "# Takes 6-7 mins to run\n",
    "noun_counter = pd.DataFrame()\n",
    "for cat in df.main_category.unique():\n",
    "  string = ''\n",
    "  for index, row in df.iterrows():\n",
    "    if row['main_category'] == cat:\n",
    "      if row['state'] == 1:\n",
    "        string = string+' ' +row['name']\n",
    "  BoW = []\n",
    "  doc = nlp(string)\n",
    "  for token in doc:\n",
    "    if token.pos_ =='NOUN':\n",
    "      BoW.append(token.text)\n",
    "  lower_tokens = [t.lower() for t in BoW]\n",
    "  no_stops = [t for t in lower_tokens\n",
    "            if t not in stopwords.words('english')]\n",
    "  no_punct = [w for w in no_stops\n",
    "        if w.isalpha()]\n",
    "  \n",
    "  noun_counter[cat]=Counter(no_punct).most_common(10)\n",
    "\n",
    "display(noun_counter)\n",
    "      "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "488_FinalProject_alpha2 _Official_Notebook.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
